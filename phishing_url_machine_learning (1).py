# -*- coding: utf-8 -*-
"""Phishing URL Machine Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KpmrNqZm5nbluonCH2I6w4cfvgAZg4HE
"""

# Import libraries
import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report
import time
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.utils import resample
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn import svm
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.datasets import make_classification

# Read CSV dataset
df = pd.read_csv('/content/PhiUSIIL_Phishing_URL_Dataset.csv')

# Check shape of dataset
df.shape

# Information on the dataset
df.info()

# Description of the dataset
df.describe()

# Check for data duplicates
print("Duplicate rows: ", df.duplicated().sum())

# Check for missing values in each column
df.isnull().sum()

"""# Minimising and balancing the data set"""

# Balance the dataset
# Separate majority (legitimate) and minority (phishing) classes
df_majority = df[df['label'] == 1]
df_minority = df[df['label'] == 0]

# Undersample the majority class
df_majority_undersampled = resample(df_majority,
                                    replace=False,    # Do not replace samples
                                    n_samples=len(df_minority),  # To match the minority class size
                                    random_state=42)

# Combine the undersampled majority class with the minority class
df_balanced = pd.concat([df_majority_undersampled, df_minority])

# Shuffle the dataset
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# Check the class distribution
print(df_balanced['label'].value_counts())

# Minimise for better training time
# Separate label 0 and label 1 data from the balanced dataset
df_label_0 = df_balanced[df_balanced['label'] == 0] # Phishing
df_label_1 = df_balanced[df_balanced['label'] == 1] # Legitimate

# Sample 10% of each class
df_label_0_sampled = df_label_0.sample(frac=0.1, random_state=42)  # 10% of label 0
df_label_1_sampled = df_label_1.sample(frac=0.1, random_state=42)  # 10% of label 1

# Combine the sampled data back together
df_new = pd.concat([df_label_0_sampled, df_label_1_sampled])

# Shuffle the dataset
df_new = df_new.sample(frac=1, random_state=42).reset_index(drop=True)

# Check the class distribution
print(df_new['label'].value_counts())

# Drop columns providing no information
df_new= df_new.drop(columns=['FILENAME'])

# Drop columns highly correlated with each other based of heatmap
df_new = df_new.drop(columns=[
    'LetterRatioInURL',
    'DegitRatioInURL',
    'DomainLength',
    'NoOfObfuscatedChar',
    'NoOfSelfRedirect',
    'DomainTitleMatchScore',
    'URLTitleMatchScore'
])

# Check shape of dataset
df_new.shape

# Label encode columns with text
# Assign the label encoder to a variable
label_encoder = LabelEncoder()

# Create a new label endoded DataFrame
le_df = df_new.copy()

# For statement: for every column with strings the label encoder will be applied
for col in le_df.select_dtypes(include=['object']).columns:
    le_df[col] = label_encoder.fit_transform(le_df[col])

# Check if it has worked
le_df.head()

# Use RFE to reduce number of columns
# Seperate features and target
X = le_df.drop(columns = 'label') # Features
y = le_df['label'] # Target

# Assign model
model = RandomForestClassifier()

# Initialise RFE with the model
rfe = RFE(model, n_features_to_select=40)
X_rfe = rfe.fit_transform(X, y)

# View ranking of features
ranking = pd.DataFrame({'Feature': X.columns, 'Ranking': rfe.ranking_})
print(ranking.sort_values(by='Ranking'))

# Drop columns which scored low on rfe
le_df = le_df.drop(columns=[
    'HasObfuscation',
    'NoOfAmpersandInURL',
    'HasExternalFormSubmit',
    'ObfuscationRatio',
    'IsDomainIP',
    'Crypto'
])

df_new = df_new.drop(columns=[
    'HasObfuscation',
    'NoOfAmpersandInURL',
    'HasExternalFormSubmit',
    'ObfuscationRatio',
    'IsDomainIP',
    'Crypto'
])

# Check shape of dataset
le_df.shape
df_new.shape

# Set features and target
features = le_df.drop(columns=['label'])
target = le_df['label']

# Standardise the features to have zero mean and unit variance
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Apply t-SNE
tsne = TSNE(n_components=2, random_state=42)  # 2D visualisation
tsne_results = tsne.fit_transform(features_scaled)

# Create a DataFrame to hold the t-SNE results
df_tsne = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'])

# Add the target variable to the t-SNE DataFrame for colouring the points
df_tsne['Label'] = target

# Plot the t-SNE result
plt.figure(figsize=(10, 8))
scatter = plt.scatter(df_tsne['TSNE1'], df_tsne['TSNE2'], c=df_tsne['Label'], cmap='viridis', alpha=0.6)
plt.title('t-SNE Visualisation')
plt.xlabel('TSNE1')
plt.ylabel('TSNE2')

# Add colour bar to show the labels
plt.colorbar(scatter)

# Show the plot
plt.show()

"""# Supervised - Classifying phishing and legitimate URLs

This section focuses on finding a machine learning algorithm which most accurately classifies phishing and legitimate URLs. The machine learning algorithms used are:


*   Random Forest Classifier - Feature Importance
*   XGBoost
*   Light GBM
*   Gaussian Naive Bayes
*   Decision Tree
*   SVM
*   k - Nearest Neighbor
*   Passive Aggressive










"""

# Random Forest Classifier
# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Labels

# Split into 70% training and 30% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialise the Random Forest Classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Start timer for training time
start_time = time.time()

# Train the model on the dataset
clf.fit(X, y)

# End timer and display results
end_time = time.time()
training_time = end_time - start_time
print(f"Training Time: {training_time} seconds")

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Print a report for more information
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Get feature importances
feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False) # Sort descending

# Display feature importances
print("Feature Importances:")
print(feature_imp)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# Creating a bar plot
sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualising Important Features")
plt.show()

# Random Forest Classifier after dropping features with less importance

# Split data into features and target
X = le_df.drop(columns=['label','NoOfURLRedirect', 'TLDLength', 'Bank', 'HasTitle', 'IsResponsive', 'HasPasswordField', 'HasHiddenFields', 'Robots', 'Pay', 'NoOfPopup', 'TLD', 'Title', 'TLDLegitimateProb', 'NoOfEqualsInURL', 'NoOfQMarkInURL'])  # Features
y = le_df['label']  # Labels

# Split into 70% training and 30% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialise the Random Forest Classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Start timer for training time
start_time = time.time()

# Train the model on the dataset
clf.fit(X, y)

# End timer and display results
end_time = time.time()
training_time = end_time - start_time
print(f"Training Time: {training_time} seconds")

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Print a report for more information
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Get feature importances
feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False) # Sort descending

# Display feature importances
print("Feature Importances:")
print(feature_imp)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# XGBoost

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Split into 70% training and 30% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Training time
start_time = time.time()

# Initialise the XGBoost Classifier

# Fit model
model = XGBClassifier()
model.fit(X_train, y_train)

# Training time
end_time = time.time()
training_time = end_time - start_time
print(f"Training Time: {training_time} seconds")

# Print model
print(model)

# Make predictions for test data
y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]

# Print a report for more information
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# LightGBM

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Split into 70% training and 30% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialise the LightGBM model
model = LGBMClassifier(random_state=42)

# Start time
start_time = time.time()

# Fit model on the training data
model.fit(X_train, y_train)

# End timer
end_time = time.time()
training_time = end_time - start_time

# Print training time
print(f"Training Time: {training_time} seconds")

# Make predictions on the test data
y_pred = model.predict(X_test)

# Print a report for more information
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Gaussian Naive Bayes

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Split into 70% training and 30% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model
model = GaussianNB()

# Check training time
start_time = time.time()

# Fit model
model.fit(X_train,y_train)

# Training time end
end_time = time.time()
training_time = end_time - start_time
print(f"Training Time: {training_time} seconds")

# Make predictions
y_pred = model.predict(X_test)

# Print a report for more information
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Decision Tree

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Fitting the model
clf = tree.DecisionTreeClassifier(criterion = 'entropy')

# Start timer
start_time = time.time()

# Fit the model
clf = clf.fit(X, y)

# End timer
end_time = time.time()
training_time = end_time - start_time

# Print training time
print(f"Training Time: {training_time} seconds")

# Visualise the tree using tree.plot_tree
tree.plot_tree(clf)

import graphviz
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data)
graph

# The predictions are stored in X_pred
X_pred = clf.predict(X)

# Verifying if the model has predicted it all right.
X_pred == y

# Compare the predictions (X_pred) with the true labels (y)
accuracy = accuracy_score(y, X_pred)

# Print a report for more information
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# SVM

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Split into 70% training and 30% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create model
clf = svm.SVC()

# Start time
start_time = time.time()

# Train model on data
clf.fit(X_train, y_train)

# End time
end_time = time.time()
training_time = end_time - start_time

# Print training time
print(f"Training Time: {training_time} seconds")

# Make predictions on test data
y_pred = clf.predict(X_test)

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Print a report for more information
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# k - Nearest Neighbor

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# List to store average cross-validation scores for each value of K
avg_scores = []

# Values of K
k_values = list(range(1, 31))

# Start time
start_time = time.time()

# Perform cross-validation for each value of K
for k in k_values:
    # Create KNN classifier with current value of K
    knn = KNeighborsClassifier(n_neighbors=k)

    # Perform cross-validation and store the mean score
    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
    avg_scores.append(scores.mean())

# End time
end_time = time.time()
training_time = end_time - start_time

# Print training time
print(f"Training Time: {training_time} seconds")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(k_values, avg_scores, marker='o')
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-Validated Accuracy')
plt.title('KNN Cross-Validation')

# Find the value of K that gives the highest mean accuracy
best_k = k_values[avg_scores.index(max(avg_scores))]
best_accuracy = max(avg_scores)

# Print scores
print(f'The best value of k is {best_k} with an accuracy of {best_accuracy}')

plt.show(), best_k

# Split data into training and test sets (e.g., 70%-30% split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the KNN model with the best K on the training set
best_knn = KNeighborsClassifier(n_neighbors=best_k)
best_knn.fit(X_train, y_train)

# Make predictions on the test set
y_pred = best_knn.predict(X_test)

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Passive Aggressive

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the Passive-Aggressive Classifier
pa_classifier = PassiveAggressiveClassifier(max_iter=1000, random_state=42)

# Start timer
start_time = time.time()

# Fit the model on the training data
pa_classifier.fit(X_train, y_train)

# End timer
end_time = time.time()
training_time = end_time - start_time

# Print training time
print(f"Training Time: {training_time} seconds")

# Predict on the test data
y_pred = pa_classifier.predict(X_test)

# Evaluate the performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""# Unsupervised - Clustering

This section looks at clustering URLs to find patterns and URLs which are bordeline classified. This can help evaluate the similarities and differences in bordeline cases. The machine learning algorithms used are:


*   K-Means ++ with elbow method
*   Deep embedding clustering
*   SOM model
*   DBSCAN
*   Hierarchical Clustering







"""

# Import libraries
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.datasets import make_blobs
!pip install minisom
from minisom import MiniSom
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster import hierarchy
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import pandas as pd

# The elbow method

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Empty list to hold scores
scores = []

# For statement to add the scores to the list
for i in range (1,11):
  kmeans = KMeans(n_clusters = i, init = 'k-means++')
  kmeans.fit(X)
  scores.append(kmeans.inertia_)

# Plot the scores
plt.plot(range(1,11), scores)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Scores')
plt.show()

# K-Means++ using PCA

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Choosing between the best number of clusters - 3 was revealed by the elbow method

# Scale the features for better clustering performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform k-Means clustering with K=3 using K-means++
kmeans_3 = KMeans(n_clusters=3, init='k-means++', random_state=42)
clusters_3 = kmeans_3.fit_predict(X_scaled)

# Evaluate clustering performance for K=3
silhouette_3 = silhouette_score(X_scaled, clusters_3)
ari_3 = adjusted_rand_score(y, clusters_3)

print(f"Silhouette Score for K=3: {silhouette_3}")
print(f"Adjusted Rand Index (ARI) for K=3: {ari_3}")

# Map clusters to actual labels
df_new['Cluster_3'] = clusters_3

# Display cluster vs true label
print(pd.crosstab(df_new['Cluster_3'], y, rownames=['Cluster'], colnames=['True Label']))

# Perform k-Means clustering with K=2 using K-means++
kmeans_2 = KMeans(n_clusters=2, init='k-means++', random_state=42)
clusters_2 = kmeans_2.fit_predict(X_scaled)

# Evaluate clustering performance for K=2
silhouette_2 = silhouette_score(X_scaled, clusters_2)
ari_2 = adjusted_rand_score(y, clusters_2)

print(f"Silhouette Score for K=2: {silhouette_2}")
print(f"Adjusted Rand Index (ARI) for K=2: {ari_2}")

# Add clusters to DataFrame
df_new['Cluster_2'] = clusters_2

# Display cluster vs true label mapping for K=2
print(pd.crosstab(df_new['Cluster_2'], y, rownames=['Cluster'], colnames=['True Label']))

# Visualising clusters using PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# This shows K=3
plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_3, cmap='viridis', marker='o', alpha=0.7)
plt.title("Clusters (K=3) Visualisation")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label="Cluster")
plt.show()

# This shows K=2
plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_2, cmap='viridis', marker='o', alpha=0.7)
plt.title("Clusters (K=2) Visualisation")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label="Cluster")
plt.show()

# K-Means++

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Choosing between the best number of clusters as 3 was revealed by the elbow method yet trialing more in case more patterns are revealed
# Scale the features for better clustering performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform k-Means clustering for K=2 to K=5 using K-means++
k_values = [2, 3, 4, 5]
clusters_dict = {}

# Perform clustering and evaluate for each K
for k in k_values:
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
    clusters = kmeans.fit_predict(X_scaled)

    # Store the clusters
    clusters_dict[k] = clusters

    # Evaluate clustering performance for current K
    silhouette = silhouette_score(X_scaled, clusters)
    ari = adjusted_rand_score(y, clusters)

    print(f"Silhouette Score for K={k}: {silhouette:.4f}")
    print(f"Adjusted Rand Index (ARI) for K={k}: {ari:.4f}")

    # Add clusters to DataFrame
    df_new[f'Cluster_{k}'] = clusters

    # Display cluster vs true label mapping
    print(pd.crosstab(df_new[f'Cluster_{k}'], y, rownames=['Cluster'], colnames=['True Label']))

# Perform t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# Plotting for each K
for k in k_values:
    plt.figure(figsize=(10, 6))
    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters_dict[k], cmap='viridis', marker='o', alpha=0.7)
    plt.title(f"Clusters (K={k}) Visualisation")
    plt.xlabel("t-SNE Component 1")
    plt.ylabel("t-SNE Component 2")
    plt.colorbar(label="Cluster")
    plt.show()

# Select two features for visualisation
feature_1 = X.columns[4]  # First feature - URL Similarity Index
feature_2 = X.columns[40]  # Second feature - No Of External Referrals

# Scatter plot for K=3 clustering
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(
    df_new[feature_1],
    df_new[feature_2],
    c=df_new['Cluster_3'],
    cmap='viridis',
    alpha=0.6
)
plt.title('K=3 Clusters')
plt.xlabel(feature_1)
plt.ylabel(feature_2)
plt.ylim(0,100)

# Scatter plot for K=2 clustering
plt.subplot(1, 2, 2)
plt.scatter(
    df_new[feature_1],
    df_new[feature_2],
    c=df_new['Cluster_2'],
    cmap='viridis',
    alpha=0.6
)
plt.title('K=2 Clusters')
plt.xlabel(feature_1)
plt.ylabel(feature_2)
plt.ylim(0,100)

plt.tight_layout()
plt.show()

#DBSCAN

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Scale the features for better clustering performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)  # eps and min_samples are hyperparameters to be tuned
clusters_dbscan = dbscan.fit_predict(X_scaled)

# Evaluate clustering performance
silhouette_dbscan = silhouette_score(X_scaled, clusters_dbscan)
ari_dbscan = adjusted_rand_score(y, clusters_dbscan)

print(f"Silhouette Score for DBSCAN: {silhouette_dbscan}")
print(f"Adjusted Rand Index (ARI) for DBSCAN: {ari_dbscan}")

# Add clusters to DataFrame
df_dbscan = le_df.copy()
df_dbscan['Cluster_DBSCAN'] = clusters_dbscan

# Display cluster vs true label mapping
print(pd.crosstab(df_dbscan['Cluster_DBSCAN'], y, rownames=['Cluster'], colnames=['True Label']))

# Visualising clusters using PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Plot DBSCAN clustering results
plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_dbscan, cmap='viridis', marker='o', alpha=0.7)
plt.title("DBSCAN Clustering Visualisation")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label="Cluster")
plt.show()

#DBSCAN using t-SNE for visualisation

# Split data into features and target
X = le_df.drop(columns=['label'])  # Features
y = le_df['label']  # Target

# Scale the features for better clustering performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters_dbscan = dbscan.fit_predict(X_scaled)

# Evaluate clustering performance
silhouette_dbscan = silhouette_score(X_scaled, clusters_dbscan)
ari_dbscan = adjusted_rand_score(y, clusters_dbscan)

print(f"Silhouette Score for DBSCAN: {silhouette_dbscan}")
print(f"Adjusted Rand Index (ARI) for DBSCAN: {ari_dbscan}")

# Add clusters to DataFrame
df_dbscan = le_df.copy()
df_dbscan['Cluster_DBSCAN'] = clusters_dbscan

# Display cluster vs true label mapping
print(pd.crosstab(df_dbscan['Cluster_DBSCAN'], y, rownames=['Cluster'], colnames=['True Label']))

# Visualising clusters using t-SNE
tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# Plot DBSCAN clustering results with t-SNE
plt.figure(figsize=(10, 6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters_dbscan, cmap='viridis', marker='o', alpha=0.7)
plt.title("DBSCAN Clustering Visualisation using t-SNE")
plt.xlabel("t-SNE Component 1")
plt.ylabel("t-SNE Component 2")
plt.colorbar(label="Cluster")
plt.show()

# Deep embedding clustering

# Splitting the dataset
X = le_df.drop(columns=['label']).values  # Features
true_labels = le_df['label'].values  # The actual label

# Define an Autoencoder for learning embeddings
class Autoencoder(nn.Module):
    def __init__(self, input_dim, embed_dim):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Linear(128, embed_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(embed_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 512),
            nn.ReLU(),
            nn.Linear(512, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Hyperparameters
input_dim = X.shape[1]
embed_dim = 2
epochs = 100
batch_size = 64
lr = 1e-3

# Convert data to torch tensor
X_tensor = torch.tensor(X, dtype=torch.float32)

# Initialise the autoencoder model, loss function and optimiser
model = Autoencoder(input_dim, embed_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# Training loop
for epoch in range(epochs):
    for i in range(0, X_tensor.size(0), batch_size):
        batch = X_tensor[i:i+batch_size]
        optimizer.zero_grad()
        output = model(batch)
        loss = criterion(output, batch)
        loss.backward()
        optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# Get the learned embeddings
embeddings = model.encoder(X_tensor).detach().numpy()

# Perform clustering using KMeans
kmeans = KMeans(n_clusters=5, random_state=42)
predicted_labels = kmeans.fit_predict(embeddings)

# Silhouette Score
sil_score = silhouette_score(embeddings, predicted_labels)
print(f"Silhouette Score: {sil_score:.4f}")

# Adjusted Rand Index (ARI)
ari_score = adjusted_rand_score(true_labels, predicted_labels)
print(f"Adjusted Rand Index (ARI): {ari_score:.4f}")

# Plot the clusters and cluster centers
plt.figure(figsize=(8, 6))
plt.scatter(embeddings[:, 0], embeddings[:, 1], c=predicted_labels, cmap='viridis', alpha=0.6)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=100, label='Cluster Centers')
plt.title("Deep Embedding Clustering")
plt.xlabel("Embedding Dimension 1")
plt.ylabel("Embedding Dimension 2")
plt.legend()
plt.show()

# Self organising maps

# Splitting the data frame
X = le_df.drop(columns=['label']).values  # Features
true_labels = le_df['label'].values  # The actual cluster labels

# Hyperparameters for SOM
som_dim = 10  # Dimension of the SOM grid (10x10 grid)
learning_rate = 0.5
sigma = 1.0
epochs = 100

# Create the SOM model
som = MiniSom(som_dim, som_dim, input_len=X.shape[1], sigma=sigma, learning_rate=learning_rate)

# Initialise the SOM weights
som.random_weights_init(X)

# Train the SOM
som.train_batch(X, epochs)

# Get the winning neurons for each data point
win_map = som.win_map(X)
winning_nodes = np.array([som.winner(x) for x in X])

# Assign cluster labels based on the SOM grid coordinates
cluster_labels = [som.get_weights()[x[0], x[1]] for x in winning_nodes]
cluster_labels = np.array(cluster_labels)

# Perform clustering (KMeans on the winning nodes)
kmeans = KMeans(n_clusters=len(np.unique(true_labels)), random_state=42)
predicted_labels = kmeans.fit_predict(cluster_labels)

# Silhouette Score
sil_score = silhouette_score(cluster_labels, predicted_labels)
print(f"Silhouette Score for SOM: {sil_score:.4f}")

# Adjusted Rand Index
ari_score = adjusted_rand_score(true_labels, predicted_labels)
print(f"Adjusted Rand Index (ARI) for SOM: {ari_score:.4f}")

# Visualising the SOM distance map
plt.figure(figsize=(10, 8))
plt.imshow(som.distance_map().T, cmap='bone')
plt.colorbar()
plt.title("SOM Distance Map")
plt.show()

# Visualise the clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=predicted_labels, cmap='viridis', alpha=0.6)
plt.title("SOM Clusters")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.ylim(0,500)
plt.show()

# Hierarchical Clustering - Bottom up

# Splitting the data frame
X = le_df.drop(columns=['label']).values  # Features
true_labels = le_df['label'].values  # The actual cluster labels

# Hierarchical clustering
n_clusters = 3
model = AgglomerativeClustering(n_clusters=n_clusters)
model = model.fit(X)
labels = model.labels_

# Linkage and dendrogram
Z = hierarchy.linkage(X, 'ward')
plt.figure(figsize=(20, 10))
dn = hierarchy.dendrogram(Z)

# Compute ARI
ari_score = adjusted_rand_score(true_labels, labels)
print(f"Adjusted Rand Index (ARI) for Agglomerative Clustering: {ari_score:.4f}")

# Compute Silhouette Score
sil_score = silhouette_score(X, labels)
print(f"Silhouette Score for Agglomerative Clustering: {sil_score:.4f}")

"""# Unsupervised - anomaly detection

Uses machine learning algorithms to find anomalies for further inspection. It solves the problem of saving time and being able to inspect unusual URLs. The machine learning algorithms used are :
*   Isolation Forest with Word2Vec
*   Isolation Forest by itself
*   VAE (Variational Autoenconders)
*   VAE with Word2Vec






"""

# Import libraries
!pip install umap-learn
from gensim.models import Word2Vec
from sklearn.ensemble import IsolationForest
from urllib.parse import urlparse
import torch
import torch.nn as nn
import torch.optim as optim
import umap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score, davies_bouldin_score
from sklearn.neighbors import LocalOutlierFactor
from scipy.spatial.distance import mahalanobis

# Isolation Forest with Word2Vec

# Tokenizing URL into parts
def tokenize_url(url):
    parsed_url = urlparse(url)
    tokens = []
    tokens.append(parsed_url.netloc)  # domain
    tokens.extend(parsed_url.path.split('/'))  # path
    if parsed_url.query:
        tokens.extend(parsed_url.query.split('&'))
    return tokens

# Tokenize all URLs in the DataFrame
tokenized_urls = df_new['URL'].apply(tokenize_url)

# Train Word2Vec Model
model = Word2Vec(tokenized_urls, vector_size=50, window=3, min_count=1, workers=4)

# Generate URL embeddings by averaging token embeddings
def get_url_embedding(url_tokens, model):
    embeddings = [model.wv[token] for token in url_tokens if token in model.wv]
    if len(embeddings) == 0:
        return np.zeros(model.vector_size)
    return np.mean(embeddings, axis=0)

url_embeddings = np.array([get_url_embedding(url_tokens, model) for url_tokens in tokenized_urls])

# Anomaly Detection using Isolation Forest
iso_forest = IsolationForest(contamination=0.2)
iso_forest.fit(url_embeddings)

# Predict anomalies
predictions = iso_forest.predict(url_embeddings)  # -1 for anomalies, 1 for normal
df_new['anomaly'] = ['Yes' if prediction == -1 else 'No' for prediction in predictions]

# UMAP application
umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
reduced_embeddings = umap_reducer.fit_transform(url_embeddings)

# Visualisation
plt.figure(figsize=(10, 8))

# Separate normal and anomalous URLs
normal_embeddings = reduced_embeddings[df_new['anomaly'] == 'No']
anomalous_embeddings = reduced_embeddings[df_new['anomaly'] == 'Yes']

# Plot normal URLs
plt.scatter(normal_embeddings[:, 0], normal_embeddings[:, 1], label='Normal', alpha=0.7, c='blue')

# Plot anomalies
plt.scatter(anomalous_embeddings[:, 0], anomalous_embeddings[:, 1], label='Anomalies', alpha=0.7, c='red')

plt.title('Word2Vec and Isolation Forest UMAP')
plt.xlabel('UMAP Dimension 1')
plt.ylabel('UMAP Dimension 2')
plt.legend()
plt.show()

# Calculate Silhouette Score

# Silhouette Score: No = Normal, Yes = Anomaly
labels = df_new['anomaly'].apply(lambda x: 0 if x == 'No' else 1).values  # Convert anomalies/normal to 0/1
silhouette = silhouette_score(url_embeddings, labels)
print(f"Silhouette Score: {silhouette}")

# Isolation Forest by itself

# Extracting features from URL
def extract_url_features(url):
    parsed_url = urlparse(url)
    features = []

    # Length of the domain name
    domain = parsed_url.netloc
    features.append(len(domain))

    # Length of the path
    path = parsed_url.path
    features.append(len(path))

    # Number of query parameters
    query = parsed_url.query
    features.append(query.count('&') + 1 if query else 0)

    # Length of the query string
    features.append(len(query))

    # Number of subdomains - split by '.'
    subdomains = domain.split('.')
    features.append(len(subdomains) - 1)  # Subdomains count

    return features

# Apply feature extraction to all URLs
url_features = np.array([extract_url_features(url) for url in df_new['URL']])

# Standardize the features
scaler = StandardScaler()
url_features_scaled = scaler.fit_transform(url_features)

# Anomaly Detection using Isolation Forest
iso_forest = IsolationForest(contamination=0.2, random_state=42)
iso_forest.fit(url_features_scaled)

# Predict anomalies
predictions = iso_forest.predict(url_features_scaled)  # -1 for anomalies, 1 for normal
df_new['anomaly'] = ['Yes' if prediction == -1 else 'No' for prediction in predictions]

# Dimensionality Reduction using UMAP
umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
reduced_embeddings = umap_reducer.fit_transform(url_features_scaled)

# Visualisation
plt.figure(figsize=(10, 8))

# Separate normal and anomalous URLs
normal_embeddings = reduced_embeddings[df_new['anomaly'] == 'No']
anomalous_embeddings = reduced_embeddings[df_new['anomaly'] == 'Yes']

# Plot normal URLs
plt.scatter(normal_embeddings[:, 0], normal_embeddings[:, 1], label='Normal', alpha=0.7, c='blue')

# Plot anomalies
plt.scatter(anomalous_embeddings[:, 0], anomalous_embeddings[:, 1], label='Anomalies', alpha=0.7, c='red')

plt.title('Isolation Forest UMAP')
plt.xlabel('UMAP Dimension 1')
plt.ylabel('UMAP Dimension 2')
plt.legend()
plt.show()

# Calculate Silhouette Score
# Silhouette Score - No = Normal, Yes = Anomaly
labels = df_new['anomaly'].apply(lambda x: 0 if x == 'No' else 1).values  # Convert anomalies/normal to 0/1
silhouette = silhouette_score(url_features_scaled, labels)
print(f"Silhouette Score: {silhouette}")

# VAE

# Splitting the dataset
X = le_df.drop(columns=['label']).values  # Features
y = le_df['label'].values  # Extract the target column

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert to PyTorch tensor
X_tensor = torch.tensor(X_scaled, dtype=torch.float32)

# Define VAE
class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc21 = nn.Linear(hidden_dim, latent_dim)  # Mean of the latent space
        self.fc22 = nn.Linear(hidden_dim, latent_dim)  # Log variance of the latent space
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h1 = torch.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        h3 = torch.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3)) # Output layer activation changed to sigmoid

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

    def loss_function(self, recon_x, x, mu, logvar):
        MSE = torch.sum((recon_x - x) ** 2)
        return MSE

# Initialise and train the VAE
input_dim = X_tensor.shape[1]
hidden_dim = 128
latent_dim = 2
vae = VAE(input_dim, hidden_dim, latent_dim)

optimizer = optim.Adam(vae.parameters(), lr=1e-3)

# Training loop
num_epochs = 20
for epoch in range(num_epochs):
    vae.train()
    optimizer.zero_grad()
    recon_batch, mu, logvar = vae(X_tensor)
    loss = vae.loss_function(recon_batch, X_tensor, mu, logvar)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Compute reconstruction errors for anomaly detection
vae.eval()
with torch.no_grad():
    recon_batch, mu, logvar = vae(X_tensor)
    recon_error = torch.mean((X_tensor - recon_batch) ** 2, dim=1)

# Set a threshold for anomaly detection
threshold = np.percentile(recon_error.numpy(), 95)
print(f"Anomaly detection threshold: {threshold}")

# Detect anomalies
anomalies = recon_error.numpy() > threshold
print("Detected anomalies:", np.where(anomalies)[0])

# Apply UMAP to the latent space for visualization
umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
latent_space_points = mu.detach().numpy()  # Use the latent space representations (mu)

reduced_embeddings = umap_reducer.fit_transform(latent_space_points)

# Visualisation
plt.figure(figsize=(10, 8))

# Separate normal and anomalous URLs
normal_embeddings = reduced_embeddings[anomalies == 0]
anomalous_embeddings = reduced_embeddings[anomalies == 1]

# Plot normal URLs
plt.scatter(normal_embeddings[:, 0], normal_embeddings[:, 1], label='Normal', alpha=0.7, c='blue')

# Plot anomalies
plt.scatter(anomalous_embeddings[:, 0], anomalous_embeddings[:, 1], label='Anomalies', alpha=0.7, c='red')

plt.title('VAE UMAP')
plt.xlabel('UMAP Dimension 1')
plt.ylabel('UMAP Dimension 2')
plt.legend()
plt.show()

# Calculate Silhouette Score
labels = np.array([1 if anomaly == 1 else 0 for anomaly in anomalies])  # 0 = Normal, 1 = Anomaly
silhouette = silhouette_score(latent_space_points, labels)
print(f"Silhouette Score: {silhouette}")

# MSE for the entire dataset
mse = torch.mean((X_tensor - recon_batch) ** 2)
print(f"Mean Squared Error for the entire dataset: {mse.item()}")

# Summary of anomaly counts
print(f"Number of anomalies detected: {np.sum(anomalies)}")

# VAE with Word2Vec

# Tokenize URLs for Word2Vec
def tokenize_url(url):
    parsed_url = urlparse(url)
    tokens = []
    tokens.append(parsed_url.netloc)  # domain
    tokens.extend(parsed_url.path.split('/'))  # path
    if parsed_url.query:
        tokens.extend(parsed_url.query.split('&'))
    return tokens


# Tokenizing the URLs in df_new - dataset with words
tokenized_urls = df_new['URL'].apply(tokenize_url)

# Train Word2Vec Model on the tokenized URLs
model = Word2Vec(tokenized_urls, vector_size=50, window=3, min_count=1, workers=4)

# Generate URL embeddings by averaging token embeddings
def get_url_embedding(url_tokens, model):
    embeddings = [model.wv[token] for token in url_tokens if token in model.wv]
    if len(embeddings) == 0:
        return np.zeros(model.vector_size)
    return np.mean(embeddings, axis=0)

# Apply Word2Vec to get embeddings for all URLs
url_embeddings = np.array([get_url_embedding(url_tokens, model) for url_tokens in tokenized_urls])

# Standardize the URL embeddings
scaler = StandardScaler()
X_scaled = scaler.fit_transform(url_embeddings)

# Convert to PyTorch tensor
X_tensor = torch.tensor(X_scaled, dtype=torch.float32)

# Define VAE
class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc21 = nn.Linear(hidden_dim, latent_dim)  # Mean of the latent space
        self.fc22 = nn.Linear(hidden_dim, latent_dim)  # Log variance of the latent space
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h1 = torch.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        h3 = torch.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))  # Output layer activation changed to sigmoid

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

    def loss_function(self, recon_x, x, mu, logvar):
        MSE = torch.sum((recon_x - x) ** 2)
        return MSE


# Initialise and train the VAE
input_dim = X_tensor.shape[1]
hidden_dim = 128
latent_dim = 2
vae = VAE(input_dim, hidden_dim, latent_dim)

optimizer = optim.Adam(vae.parameters(), lr=1e-3)

# Training loop
num_epochs = 20
for epoch in range(num_epochs):
    vae.train()
    optimizer.zero_grad()
    recon_batch, mu, logvar = vae(X_tensor)
    loss = vae.loss_function(recon_batch, X_tensor, mu, logvar)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Compute reconstruction errors for anomaly detection
vae.eval()
with torch.no_grad():
    recon_batch, mu, logvar = vae(X_tensor)
    recon_error = torch.mean((X_tensor - recon_batch) ** 2, dim=1)

# Set a threshold for anomaly detection
threshold = np.percentile(recon_error.numpy(), 95)
print(f"Anomaly detection threshold: {threshold}")

# Detect anomalies
anomalies = recon_error.numpy() > threshold
print("Detected anomalies:", np.where(anomalies)[0])

# Apply UMAP to the latent space for visualisation
umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
latent_space_points = mu.detach().numpy()  # Use the latent space representations (mu)

reduced_embeddings = umap_reducer.fit_transform(latent_space_points)

# Visualisation
plt.figure(figsize=(10, 8))

# Separate normal and anomalous URLs
normal_embeddings = reduced_embeddings[anomalies == 0]
anomalous_embeddings = reduced_embeddings[anomalies == 1]

# Plot normal URLs
plt.scatter(normal_embeddings[:, 0], normal_embeddings[:, 1], label='Normal', alpha=0.7, c='blue')

# Plot anomalies
plt.scatter(anomalous_embeddings[:, 0], anomalous_embeddings[:, 1], label='Anomalies', alpha=0.7, c='red')

plt.title('VAE UMAP with Word2Vec')
plt.xlabel('UMAP Dimension 1')
plt.ylabel('UMAP Dimension 2')
plt.legend()
plt.show()

# Silhouette Score
labels = np.array([1 if anomaly == 1 else 0 for anomaly in anomalies])  # 0 = Normal, 1 = Anomaly
silhouette = silhouette_score(latent_space_points, labels)
print(f"Silhouette Score: {silhouette}")

# MSE for the entire dataset
mse = torch.mean((X_tensor - recon_batch) ** 2)
print(f"Mean Squared Error for the entire dataset: {mse.item()}")

# Summary of anomaly counts
print(f"Number of anomalies detected: {np.sum(anomalies)}")

"""# Visualising results"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Problem 1 results
data = {
    'Model': ['XGBOOST', 'LIGHT GBM', 'RF', 'GNB', 'DT', 'KNN', 'PA', 'SVM'],
    'Accuracy': [1.00, 1.00, 1.00, 0.99, 0.99, 0.99, 0.98, 0.97],
    'Precision': [1.00, 1.00, 1.00, 0.99, 0.99, 0.99, 0.98, 0.97],
    'Recall': [1.00, 1.00, 1.00, 0.99, 0.99, 0.99, 0.98, 0.97],
    'F1-Score': [1.00, 1.00, 1.00, 0.99, 0.99, 0.99, 0.98, 0.97],
    'Training Time': [0.302, 0.433, 1.791, 0.019, 0.093, 125.246, 0.218, 3.424],
    'False Negatives': [0, 0, 0, 1, 1, 26, 80, 182],
    'False Positives': [0, 0, 0, 77, 77, 11, 43, 9],
}

p1_results = pd.DataFrame(data)

# Training Time Comparison
plt.figure(figsize=(8, 5))
sns.barplot(x='Model', y='Training Time', data=p1_results)
plt.title('Training Time Comparison')
plt.ylabel('Time (seconds)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Confusion Matrix Misclassification
misclassified = p1_results[['Model', 'False Positives', 'False Negatives']].melt('Model', var_name='Type', value_name='Count')
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='Count', hue='Type', data=misclassified)
plt.title('False Positives and False Negatives')
plt.xticks(rotation=45)
plt.ylabel('Count')
plt.legend(title='Misclassification Type')
plt.tight_layout()
plt.show()

# Misclassified Phishing URLs
plt.figure(figsize=(8, 5))
sns.barplot(x='False Negatives', y='Model', data=p1_results, orient='h')
plt.title('Misclassified Phishing URLs (False Negatives)')
plt.xlabel('False Negatives')
plt.ylabel('Model')
plt.tight_layout()
plt.show()

# Visualising problem 2 results

import matplotlib.pyplot as plt
import numpy as np

# Data
models = ["Hierarchical Clustering", "DEC", "SOM", "K-Means ++", "DBSCAN"]
silhouette_scores = [0.9745, 0.9707, 0.8851, 0.2205, -0.2995]
ari_scores = [0.0000, 0.0001, 0.0006, 0.9635, 0.0294]

# Bar width and positions
bar_width = 0.35
x = np.arange(len(models))

# Plot
plt.figure(figsize=(10, 6))
plt.bar(x - bar_width / 2, silhouette_scores, bar_width, label="Silhouette Score", color="skyblue")
plt.bar(x + bar_width / 2, ari_scores, bar_width, label="Adjusted Rand Index (ARI)", color="orange")

# Labels and title
plt.xlabel("Clustering Models", fontsize=12)
plt.ylabel("Scores", fontsize=12)
plt.title("Comparison of Clustering Models", fontsize=14)
plt.xticks(x, models, rotation=15, ha="right")
plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)

# Legend
plt.legend()

# Display
plt.tight_layout()
plt.show()

# Data for the graph
k_values = [2, 3, 4, 5]
silhouette_scores = [0.2205, 0.2211, 0.1541, 0.1473]
ari_scores = [0.9635, 0.9070, 0.7167, 0.6501]

# Create figure and axis
fig, ax1 = plt.subplots(figsize=(8, 6))

# Plot Silhouette Scores
ax1.set_xlabel('Number of Clusters (K)')
ax1.set_ylabel('Silhouette Score', color='tab:blue')
ax1.plot(k_values, silhouette_scores, color='tab:blue', marker='o', label='Silhouette Score')
ax1.tick_params(axis='y', labelcolor='tab:blue')

# Create second y-axis for ARI Scores
ax2 = ax1.twinx()
ax2.set_ylabel('Adjusted Rand Index (ARI)', color='tab:green')
ax2.plot(k_values, ari_scores, color='tab:green', marker='s', label='ARI')
ax2.tick_params(axis='y', labelcolor='tab:green')

# Title and grid
plt.title('Silhouette Score and Adjusted Rand Index (ARI) for Different K Values')
ax1.grid(True)

# Show the plot
plt.tight_layout()
plt.show()